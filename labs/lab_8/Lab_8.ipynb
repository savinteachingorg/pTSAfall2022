{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 3001.001 Special Topics in Data Science: Probabilistic Time Series Analysis\n",
    "\n",
    "# Lab 8 Hidden Markov Model\n",
    "\n",
    "### Please turn in the code before 11/07/2020 at 11:59 pm. Please name your notebook netid.ipynb.\n",
    "\n",
    "### Your work will be evaluated based on the code and plots. You don't need to write down your answers to these questions in the text blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import pylab\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load the Wall Street Journal POS dataset. Transform them into indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"en-wsj-train.pos\"\n",
    "test_dir = \"en-wsj-dev.pos\"\n",
    "\n",
    "def load_pos_data(data_dir, word_indexer=None, label_indexer=None, top_k=20000):\n",
    "    \"\"\"\n",
    "    Function that loads the data\n",
    "    \"\"\"\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        # load data\n",
    "        content = f.readlines()\n",
    "        intermediate_X = []\n",
    "        intermediate_y = []\n",
    "        current_X = []\n",
    "        current_y = []\n",
    "        vocab_counter = Counter()\n",
    "        label_set = set()\n",
    "        for line in content:\n",
    "            tokens = line.replace(\"\\n\", \"\").replace(\"$\", \"\").split(\"\\t\")\n",
    "            # !!! if problems use line below\n",
    "            # tokens = line.replace(\"/n\", \"\").replace(\"$\", \"\").split(\"/t\")\n",
    "            if len(tokens) <= 1:\n",
    "                intermediate_X.append(current_X)\n",
    "                intermediate_y.append(current_y)\n",
    "                current_X = []\n",
    "                current_y = []\n",
    "            elif len(tokens[1]) > 0:\n",
    "                vocab_counter[tokens[0]]+=1\n",
    "                label_set.add(tokens[1])\n",
    "                current_X.append(tokens[0].lower())\n",
    "                current_y.append(tokens[1])\n",
    "        # index data\n",
    "        top_k_words = vocab_counter.most_common(top_k)\n",
    "        # 0 is reserved for unknown words\n",
    "        word_indexer = word_indexer if word_indexer is not None else dict([(top_k_words[i][0], i+1) for i in range(len(top_k_words))])\n",
    "        word_indexer[\"UNK\"] = 0 \n",
    "        label_indexer = label_indexer if label_indexer is not None else dict([(label, i) for i, label in enumerate(label_set)])\n",
    "        output_X = []\n",
    "        output_y = []\n",
    "        current_X = []\n",
    "        current_y = []\n",
    "        \n",
    "        for i in range(len(intermediate_X)):\n",
    "            for j in range(len(intermediate_X[i])):\n",
    "                if intermediate_X[i][j] in word_indexer:\n",
    "                    current_X.append(word_indexer[intermediate_X[i][j]])\n",
    "                else:\n",
    "                    current_X.append(0)\n",
    "                current_y.append(label_indexer[intermediate_y[i][j]])\n",
    "            # populate holders\n",
    "            output_X.append(current_X)\n",
    "            output_y.append(current_y)\n",
    "            # reset current holder\n",
    "            current_X = []\n",
    "            current_y = []\n",
    "        return output_X, output_y, label_indexer, word_indexer, {v: k for k, v in label_indexer.items()}, {v: k for k, v in word_indexer.items()}\n",
    "    \n",
    "def reconstruct_sequence(idx_list, lookup):\n",
    "    \"\"\"\n",
    "    Function that reconstructs a sequence from index\n",
    "    \"\"\"\n",
    "    return [lookup[x] for x in idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_z, label_indexer, word_indexer, label_lookup, vocab_lookup = load_pos_data(train_dir)\n",
    "test_X, test_z, _, _, _, _ = load_pos_data(test_dir, word_indexer=word_indexer, label_indexer=label_indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data description\n",
    "\n",
    "data are excerpts from the wall street journal\n",
    "\n",
    "observations are words so that sentences form time series, the observation model is multinomial\n",
    "\n",
    "we want to extract a latent space that characterizes the grammatical structure of sentences, the dataset is already labeled (for each observed word we are given the latent state which is the grammatical category the word belongs to)\n",
    "\n",
    "more about the meaning of the latent space is here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we have the following variables\n",
    "\n",
    "'train_X' = a list of lists, each list is a sentence with numbers representing words (observations)\n",
    "\n",
    "'word_indexer' and 'vocab_lookup go from words to numbers, length 20001\n",
    "\n",
    "'label_indexer' and'label_lookup' go from category label to numbers, length 44\n",
    "\n",
    "We will fit a HMM where each observed word is attributed to a category (\"current state\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example training sentence: \n",
      "\n",
      " ['a', 'record', 'date', 'has', \"n't\", 'been', 'set', '.']\n",
      "example's categories: \n",
      "\n",
      " ['DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'VBN', '.']\n"
     ]
    }
   ],
   "source": [
    "ii = 7\n",
    "print(\"example training sentence: \\n\\n\", reconstruct_sequence(train_X[ii], vocab_lookup))\n",
    "print(\"example's categories: \\n\\n\", reconstruct_sequence(train_z[7], label_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39815  training sentences\n",
      "1700 testing sentences\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X), ' training sentences')\n",
    "print(len(test_X), 'testing sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observed states $X=\\{x_1, x_2, ..., x_N\\}$ and $x_n=h\\in\\{h=1, ..., H\\}$\n",
    "\n",
    "observed variable given the latent could be for instance gaussian distributed where the latent state defines which gaussian they are from - in the case of our dataset above the observed variables are multinomial\n",
    "\n",
    "\n",
    "latent states $Z=\\{z_1, z_2, ..., z_N\\}$ and $z_n\\in\\{1, ..., K\\}$, so a discrete multinomial variable\n",
    "\n",
    "\n",
    "Transition probability matrix: $A\\in R^{KxK}$ where $A_{i,j}=p(z_n=j|z_{n-1}=i)$ and rows sum to $1$\n",
    "\n",
    "Emission probability matrix: $C\\in R^{KxH}$ where $C_{i,h}=C_{i}(x_n=h)=p(x_n=h|z_{n}=i)$ and rows sum to $1$\n",
    "\n",
    "Initial State Probability: $\\pi\\in R^K$ where we will set the initial state to one specified category so that $\\pi_i=p(z_1=i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### special cases of HMM models through constraint on transition probability matrix $A$\n",
    "- slow changes and fast changes\n",
    "- directional, eg left-to-right HMM\n",
    "- assume latent states have a natural order and make large jumps unlikely\n",
    "\n",
    "*any entry of A that is set to $0$ in the initialization will stay $0$ during the EM updates*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood (the forward algorithm)\n",
    "\n",
    "given parameters $\\theta=\\{A, C, \\pi\\}$ and observation sequence $X$ find the likelihood $p(X|\\theta)$\n",
    "\n",
    "compute the probability of being in a state $j$ after seeing the first $n$ observations, denote that probability as $\\alpha_n(j)$\n",
    "\n",
    "$$\\alpha_n(j)=P(x_1, ..., x_n, z_n=j)=P(x_n|z_n=j)\\sum_{i=1}^K \\alpha_{n-1}P(z_n=j|z_{n-1}=i)= $$\n",
    "$$C_j(x_n)\\sum_{i=1}^K\\alpha_{n-1}(i)A_{ij}$$\n",
    "\n",
    "with initial condition\n",
    "\n",
    "$$\\alpha_1(i)=P(x_1|z_1=i)P(z_1=i)=\\pi_iC_i(x_1)$$\n",
    "\n",
    "so that \n",
    "$$P(X|\\theta)=\\sum_{i=1}^K \\alpha_N(i)$$\n",
    "\n",
    "*implementation caveat: introduce scaling to avoid numerical problems due to small probability values (see lecture or Bishop)*\n",
    "\n",
    "*there is the equivalent for the other direction, called the Backward Algorithm, that uses* $$\\beta_n(j)=p(x_{n+1},...x_N|z_n=j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm (max sum algorithm)\n",
    "\n",
    "Aim: inference - estimate the latent state sequence, assume parameters are given\n",
    "\n",
    "There are exponentially many possible chains of latent spaces in a sequence. However, let's assume we are given the probabilities up to $n$ we can infer the probabilities for the next time step $n+1$. If we work our way forward recursively in this way, we can dramatically decrease the computational cost. Essentially, we only consider the path with the highest probability at every time point for each state.\n",
    "\n",
    "$v_n(j)$ is the probability that we are in state $j$ after seeing the first $n$ observations and passing through the most probable latent state sequence\n",
    "\n",
    "- initialization: \n",
    "$$v_1(j)=\\pi_j C_j(x_1)\\hspace{2in}(1)$$\n",
    "\n",
    "for every time point $n$ and every possible latent state $j=1,...,K$\n",
    "\n",
    "- update: \n",
    "$$v_n(j)=C_j(x_n)max_{i=1}^Kv_{n-1}(i)A_{ij}\\hspace{2in}(2)$$\n",
    "\n",
    "- store best state \n",
    "$$b_n(j)=argmax_{i=1}^K v_{n-1}(i)A_{ij}C_j(x_n)=argmax_{i=1}^Kv_n(j)\\hspace{2in}(3)$$\n",
    "\n",
    "Once this is done for the whole observation sequence, backtrack from the last time point ($N$) to find the estimate for the latent:\n",
    "\n",
    "$$z_n = b_n(z_{n+1})$$\n",
    "\n",
    "Where the process is initialized at the end with:\n",
    "\n",
    "$$z_N = argmax_{i=1}^K n_{N-1}(i)$$\n",
    "\n",
    "\n",
    "## *for implementation, make sure you use the log form instead (take the logarithm of the right hand side of equation (2))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter learning\n",
    "\n",
    "in our dataset the latent states $Z$ are given (E step of EM already solved), therefore we can update the parameters simply as follows:\n",
    "\n",
    "$$\\hat{A}_{ij}=\\frac{num\\ state\\ transitions\\ from\\ i\\ to\\ j}{num\\ state\\ transitions\\ from\\ i}$$\n",
    "\n",
    "$$\\hat{C}_{ih}=\\frac{num\\ of\\ times\\ state\\ i\\ emits\\ h}{num\\ state\\ i}$$\n",
    "\n",
    "$$\\hat{\\pi}_{i}=\\frac{num\\ of\\ chains\\ start\\ with\\ i}{total\\ num\\ of\\ chains}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Implementation\n",
    "\n",
    "In this part, you will implement the Hidden Markov Model with following methods:\n",
    "\n",
    "- decode_single_chain: E-step use the Viterbi Algorithm to find the most probable sequence of states.\n",
    "\n",
    "- sample: given an initial state and the number of steps, returns a sequence of sampled states and observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentage_agree(x, z):\n",
    "    \"\"\"\n",
    "    Function that shows the % of agreement among two list\n",
    "    \"\"\"\n",
    "    assert len(x)==len(z)\n",
    "    return float(np.sum(np.array(x)==np.array(z)))/len(x)\n",
    "\n",
    "class MyHMM:\n",
    "    def __init__(self, num_unique_states, num_observations):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        @param num_unique_states: # of unique states (POS Tags)\n",
    "        @param num_observations: # of unique observations (words)\n",
    "        \"\"\"\n",
    "        self.num_unique_states = num_unique_states\n",
    "        self.num_observations = num_observations\n",
    "        self.transition_matrix = np.zeros((num_unique_states, num_unique_states))\n",
    "        self.emission_matrix = np.zeros((num_unique_states, num_observations))\n",
    "        self.initial_states_vector = np.zeros(num_unique_states)\n",
    "    \n",
    "    def fit(self, X, z):\n",
    "        \"\"\"\n",
    "        Method that fits the model.\n",
    "        @param X: array-like with dimension [# of examples, # of length]\n",
    "        @param z: array-like with dimension [# of examples, # of length]\n",
    "        \"\"\"\n",
    "        # populate holders\n",
    "        for i in range(len(z)):\n",
    "            for j in range(len(z[i])):\n",
    "                # update initial state probability\n",
    "                if j == 0:\n",
    "                    self.initial_states_vector[z[i][j]] += 1\n",
    "                # update transition matrix\n",
    "                if j < len(z[i])-1:\n",
    "                    self.transition_matrix[z[i][j], z[i][j+1]] += 1\n",
    "                # update emission matrix\n",
    "                self.emission_matrix[z[i][j], X[i][j]] += 1\n",
    "        # normalization\n",
    "        self.initial_states_vector += 1e-10\n",
    "        self.initial_states_vector /= np.sum(self.initial_states_vector)\n",
    "        \n",
    "        self.transition_matrix += 1e-10\n",
    "        row_sums_transition_matrix = np.sum(self.transition_matrix, axis=1)\n",
    "        self.transition_matrix /= row_sums_transition_matrix[:, np.newaxis]\n",
    "        \n",
    "        self.emission_matrix += 1e-10\n",
    "        row_sums_emission_matrix = np.sum(self.emission_matrix, axis=1)\n",
    "        self.emission_matrix /= row_sums_emission_matrix[:, np.newaxis]\n",
    "    \n",
    "    def decode_single_chain(self, x):\n",
    "        \"\"\"\n",
    "        Auxiliary method that uses Viterbi on single chain\n",
    "        @param X: array-like with dimension [ # of length]\n",
    "        @return z: array-like with dimension [# of length]\n",
    "        \"\"\"\n",
    "        # init holders\n",
    "        z = []\n",
    "        V = np.zeros( (len(x), self.num_unique_states) )\n",
    "        best_states = np.zeros( (len(x), self.num_unique_states) )\n",
    "        \n",
    "        #########################################\n",
    "        # TODO: implement the Viterbi algorithm #\n",
    "        #########################################\n",
    "        \n",
    "        return np.zeros(len(x))\n",
    "        \n",
    "    def decode(self, X):\n",
    "        \"\"\"\n",
    "        Method that performs the Viterbi the model.\n",
    "        @param X: array-like with dimension [# of examples, # of length]\n",
    "        @return z: array-like with dimension [# of examples, # of length]\n",
    "        \"\"\"\n",
    "        return [self.decode_single_chain(sample) for sample in X]\n",
    "    \n",
    "    def sample(self, n_step, initial_state, seed=0):\n",
    "        \"\"\"\n",
    "        Method that given initial state and produces n_step states and observations\n",
    "        @param n_step: integer\n",
    "        @param initial_state: an integer indicating the state\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        observations = []\n",
    "        current_state = initial_state\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        ###############################\n",
    "        # TODO: sample from the model #\n",
    "        ###############################\n",
    "\n",
    "        return states, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn an HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_states = len(label_indexer)\n",
    "num_obs = len(word_indexer)\n",
    "my_hmm = MyHMM(num_states, num_obs)\n",
    "my_hmm.fit(train_X, train_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Viterbi to decode one single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['this', 'financing', 'system', 'was', 'created', 'in', 'the', 'new', 'law', 'in', 'order', 'to', 'keep', 'the', 'bailout', 'spending', 'from', 'swelling', 'the', 'budget', 'deficit', '.'] \n",
      "\n",
      " pred: ['VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD', 'VBD'] \n",
      "\n",
      " true label: ['DT', 'NN', 'NN', 'VBD', 'VBN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NN', 'TO', 'VB', 'DT', 'NN', 'NN', 'IN', 'VBG', 'DT', 'NN', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "i = 5 # decode the ith sentence in the testing dataset\n",
    "res = my_hmm.decode_single_chain(test_X[i])\n",
    "print(\"data: {0} \\n\\n pred: {1} \\n\\n true label: {2}\".format(reconstruct_sequence(test_X[i], vocab_lookup),\n",
    "                                                    reconstruct_sequence(res, label_lookup),\n",
    "                                                  reconstruct_sequence(test_z[i], label_lookup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decode for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takes 0.00699162483215332 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_train = my_hmm.decode(train_X[:1000])\n",
    "print(\"takes {0} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takes 0.01000833511352539 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_test = my_hmm.decode(test_X)\n",
    "print(\"takes {0} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent correct predicted in training:  0.0356558432857\n"
     ]
    }
   ],
   "source": [
    "print('percent correct predicted in training: ', np.mean([percentage_agree(pred_train[ii], train_z[ii]) for ii in range(len(pred_train))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent correct predicted in testing:  0.0459831907355\n"
     ]
    }
   ],
   "source": [
    "print('percent correct predicted in testing: ', np.mean([percentage_agree(pred_test[i], test_z[i]) for i in range(len(pred_test))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states:  []\n",
      "observed:  []\n"
     ]
    }
   ],
   "source": [
    "pos_tag, words = my_hmm.sample(10, label_indexer[\"NNP\"], seed=38)\n",
    "print('states: ',reconstruct_sequence(pos_tag, label_lookup))\n",
    "print('observed: ', reconstruct_sequence(words, vocab_lookup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please turn in the code before 11/07/2020 at 11:59 pm. Please name your notebook netid.ipynb.\n",
    "\n",
    "### Your work will be evaluated based on the code and plots. You don't need to write down your answers to these questions in the text blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
