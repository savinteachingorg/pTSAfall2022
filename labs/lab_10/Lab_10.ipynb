{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfmuZW7Mbgc3"
   },
   "source": [
    "# DS-GA 1018 Probabilistic Time Series Analysis\n",
    "\n",
    "# Lab 10 RNNs\n",
    "\n",
    "## Please turn in the code and pdf before 12/05/2022 at 11:59 pm. Please name your notebook netid.ipynb.\n",
    "\n",
    "### Your work will be evaluated based on the code and plots. You don't need to write down your answers to these questions in the text blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W5R3cIaobgdA"
   },
   "outputs": [],
   "source": [
    "### --- Basic packages we need --- ###\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from pdb import set_trace\n",
    "from scipy.signal import decimate\n",
    "\n",
    "#Utility function, IGNORE\n",
    "def split_weight_matrix(A, sizes, axis=1):\n",
    "    \"\"\"Splits a weight matrix along the specified axis (0 for row, 1 for\n",
    "    column) into a list of sub arrays of size specified by 'sizes'.\"\"\"\n",
    "\n",
    "    idx = [0] + np.cumsum(sizes).tolist()\n",
    "    if axis == 1:\n",
    "        ret = [np.squeeze(A[:,idx[i]:idx[i+1]]) for i in range(len(idx) - 1)]\n",
    "    elif axis == 0:\n",
    "        ret = [np.squeeze(A[idx[i]:idx[i+1],:]) for i in range(len(idx) - 1)]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmTZcfH6bgdC"
   },
   "source": [
    "To keep our implementation somewhat general, we'll define a Function class that is simply a way for a function *and* its derivative to be stored in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TelkL-NSbgdD"
   },
   "outputs": [],
   "source": [
    "class Function:\n",
    "    \"\"\"Defines a function and its derivative.\n",
    "    \n",
    "    Attributes:\n",
    "        f (function): An element-wise differentiable function that acts on a\n",
    "            1-d numpy array of arbitrary dimension. May include a second\n",
    "            argument for a label, e.g. for softmax-cross-entropy.\n",
    "        f_prime (function): The element-wise derivative of f with respect to\n",
    "            the first argument, must also act on 1-d numpy arrays of arbitrary\n",
    "            dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, f, f_prime):\n",
    "        \"\"\"Inits an instance of Function by specifying f and f_prime.\"\"\"\n",
    "        \n",
    "        self.f = f\n",
    "        self.f_prime = f_prime\n",
    "\n",
    "### Now we create useful instances of Function ###\n",
    "        \n",
    "#tanh\n",
    "\n",
    "def tanh_(z):\n",
    "\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "tanh = Function(tanh_, tanh_derivative)\n",
    "\n",
    "#identity\n",
    "\n",
    "def identity_(z):\n",
    "    \n",
    "    return z\n",
    "\n",
    "def identity_derivative(z):\n",
    "    \n",
    "    return np.ones_like(z)\n",
    "\n",
    "identity = Function(identity_, identity_derivative)\n",
    "\n",
    "#mean-squared error\n",
    "\n",
    "def mean_squared_error_(z, y):\n",
    "    \n",
    "    return 0.5*np.square(z - y).mean()\n",
    "\n",
    "def mean_squared_error_derivative(z, y):\n",
    "    \n",
    "    return z - y\n",
    "    \n",
    "mean_squared_error = Function(mean_squared_error_, mean_squared_error_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6pakZAVbgdE"
   },
   "source": [
    "Below is our \"RNN\" class. An instance of RNN is used to run the process *forwards*, keeping track of relevant variables in the process.\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf h}^{(t)} &= {\\bf W}^{\\text{rec}} {\\bf a}^{(t-1)} + {\\bf W}^{\\text{in}} {\\bf x}^{(t)} + {\\bf b}^{\\text{rec}}\\\\\n",
    "{\\bf a}^{(t)} &= \\phi({\\bf h}^{(t)})\n",
    "\\end{align}\n",
    "\n",
    "It also projects to an output space via\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf z}^{(t)} = {\\bf W}^{\\text{out}} {\\bf a}^{(t)} + {\\bf b}^{\\text{out}}.\n",
    "\\end{align}\n",
    "\n",
    "We call these z values the \"pre-outputs,\" since they are not themselves the actual prediction, but rather that which is passed to some function $\\psi$, e.g. sigmoid or softmax into a space of classes.(Think logits vs. actual classification probabilities.) We denote the prediction\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf y}^{(t)}_{\\text{hat}} = \\psi({\\bf z}^{(t)})\n",
    "\\end{align}\n",
    "\n",
    "as \"y_hat\" in the code. (For the purpose of this lab, we will use the identity as $\\psi$ for convenience.) Then we finally calculate the loss, but we actually do so as a function of ${\\bf z}$ (and the training label ${\\bf y}^{*(t)}$), with the output included in the loss function. This is because it's easier for certain types of losses with singularities, such as sigmoid- or softmax-cross-entropy.\n",
    "\n",
    "\\begin{align}\n",
    "L^{(t)} = L({\\bf z}^{(t)}, {\\bf y}^{*(t)})\n",
    "\\end{align}\n",
    "\n",
    "This loss *value* is denoted by \"loss_\", and the function that calculates it is \"loss\".\n",
    "\n",
    "For the purpose of learning, we must also calculate the \"Jacobian\" of the network, which we include in the RNN class because the mathematical form of the Jacobian varies by RNN architecture---remember, these update equations are just one case of a more general $F_{\\bf w}({\\bf a}^{(t-1)}, {\\bf x}^{(t)})$.\n",
    "\n",
    "\\begin{align}\n",
    "J^{(t)}_{ij} = \\partial a^{(t)}_i / \\partial a^{(t-1)}_j = \\phi'(h^{(t)}_i) W^{\\text{rec}}_{ij}\n",
    "\\end{align}\n",
    "\n",
    "TODO:\n",
    "\n",
    "1. Fill in the \"next_state\" function that updates ${\\bf h}$ and ${\\bf a}$.\n",
    "2. Fill in the \"next_output\" function that updates ${\\bf z}$ and ${\\bf y}_{\\text{hat}}$\n",
    "3. Fill in the \"get_jacobian\" function that calculates ${\\bf J}$ based on the current values of ${\\bf h}$ and ${\\bf W}^{\\text{rec}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tkNEVFsRbgdF"
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"A vanilla recurrent neural network.\n",
    "\n",
    "    Attributes:\n",
    "        n_in (int): Number of input dimensions\n",
    "        n_h (int): Number of hidden units\n",
    "        n_out (int): Number of output dimensions\n",
    "        W_in (numpy array): Array of shape (n_h, n_in), weights task inputs.\n",
    "        W_rec (numpy array): Array of shape (n_h, n_h), weights recurrent\n",
    "            inputs.\n",
    "        W_out (numpy array): Array of shape (n_out, n_h), provides hidden-to-\n",
    "            output-layer weights.\n",
    "        b_rec (numpy array): Array of shape (n_h), represents the bias term\n",
    "            in the recurrent update equation.\n",
    "        params (list): The list of each parameter's current value, in the order\n",
    "            [W_rec, W_in, b_rec, W_out, b_out].\n",
    "        activation (functions.Function): An instance of the Function class\n",
    "            used as the network's nonlinearity \\phi in the recurrent update\n",
    "            equation.\n",
    "        output (functions.Function): An instance of the Function class used\n",
    "            for calculating final output from z.\n",
    "        loss (functions.Function): An instance of the Function class used for\n",
    "            calculating loss from z (must implicitly include output function,\n",
    "            e.g. softmax_cross_entropy if output is softmax).\n",
    "        x (numpy array): Array of shape (n_in) representing the current inputs\n",
    "            to the network.\n",
    "        h (numpy array): Array of shape (n_h) representing the pre-activations\n",
    "            of the network.\n",
    "        a (numpy array): Array of shape (n_h) representing the post-activations\n",
    "            of the network.\n",
    "        z (numpy array): Array of shape (n_out) reprenting the outputs of the\n",
    "            network, before any final output nonlinearities, e.g. softmax,\n",
    "            are applied.\n",
    "        error (numpy array): Array of shape (n_out) representing the derivative\n",
    "            of the loss with respect to z. Calculated by loss.f_prime.\n",
    "        y_hat (numpy array): Array of shape (n_out) representing the final\n",
    "            outputs of the network, to be directly compared with task labels.\n",
    "        *_prev (numpy array): Array representing any of x, h, a, or z at the\n",
    "            previous time step.\"\"\"\n",
    "\n",
    "    def __init__(self, W_in, W_rec, W_out, b_rec, b_out,\n",
    "                 activation, output, loss):\n",
    "        \"\"\"Initializes an RNN by specifying its initial parameter values,\n",
    "        activation, output, and loss functions.\"\"\"\n",
    "\n",
    "        #Initial parameter values\n",
    "        self.W_in = W_in\n",
    "        self.W_rec = W_rec\n",
    "        self.W_out = W_out\n",
    "        self.b_rec = b_rec\n",
    "        self.b_out = b_out\n",
    "\n",
    "        #Network dimensions\n",
    "        self.n_in = W_in.shape[1]\n",
    "        self.n_h = W_in.shape[0]\n",
    "        self.n_out = W_out.shape[0]\n",
    "\n",
    "        #Check dimension consistency.\n",
    "        assert self.n_h == W_rec.shape[0]\n",
    "        assert self.n_h == W_rec.shape[1]\n",
    "        assert self.n_h == W_in.shape[0]\n",
    "        assert self.n_h == W_out.shape[1]\n",
    "        assert self.n_h == b_rec.shape[0]\n",
    "        assert self.n_out == b_out.shape[0]\n",
    "\n",
    "        #Define shapes and params lists for convenience later.\n",
    "        self.params = [self.W_rec, self.W_in, self.b_rec,\n",
    "                       self.W_out, self.b_out]\n",
    "\n",
    "        #Activation and loss functions\n",
    "        self.activation = activation\n",
    "        self.output = output\n",
    "        self.loss = loss\n",
    "        \n",
    "        #Initial state values\n",
    "        self.reset_network()\n",
    "\n",
    "    def reset_network(self, sigma=1):\n",
    "        \n",
    "        self.h = np.random.normal(0, sigma, self.n_h)\n",
    "        self.a = self.activation.f(self.h) #Specify activations by \\phi.\n",
    "        self.z = self.W_out.dot(self.a) + self.b_out #Specify outputs from a\n",
    "\n",
    "    def next_state(self, x):\n",
    "        \"\"\" FILL THIS FUNCTION IN \n",
    "        \n",
    "        After copying the inputs and previous state variable into network\n",
    "        attributes, update the new state variables self.a and self.h according\n",
    "        to our forward pass equation.\n",
    "        \n",
    "        Arguments:\n",
    "            x (numpy array, shape (n_in)): Input data for this time step\n",
    "            \n",
    "        Updates:\n",
    "            self.h (numpy array, shape (n_h)): pre-activations\n",
    "            self.a (numpy array, shape (n_h)): post-activations\n",
    "        \n",
    "        Returns:\n",
    "            None\"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.a_prev = np.copy(self.a)\n",
    "\n",
    "        ### TODO ###\n",
    "        # Update the new preactivation self.h using the network parameters\n",
    "        # (think self.W_rec, etc.) and the current network state/inputs\n",
    "        # (think self.x, self.h, etc.). Then update the post.activations\n",
    "        # self.a using the network nonlinearity self.activation.f.\n",
    "\n",
    "        self.h = 0\n",
    "        self.a = 0\n",
    "\n",
    "    def next_output(self):\n",
    "        \"\"\" FILL THIS FUNCTION IN \n",
    "        \n",
    "        After copying previous outputs into state variable, update the new state\n",
    "        variable using the network's output parameters and the final output\n",
    "        using the output activation function \\psi.\n",
    "        \n",
    "        Arguments:\n",
    "            None\n",
    "            \n",
    "        Updates:\n",
    "            self.z (numpy array, shape (n_out)): pre-outputs\n",
    "            self.y_hat (numpy array, shape (n_out)): final prediction\n",
    "            \n",
    "        Returns:\n",
    "            None\"\"\"\n",
    "        \n",
    "        ### TODO ###\n",
    "        ## Update the \"pre-outputs\" self.z via an (affine) linear\n",
    "        ## function of the hidden state self.a using the network's\n",
    "        ## output parameters. Then apply the output nonlinearity\n",
    "        ## to update the final prediction y_hat.\n",
    "\n",
    "        self.z = 0\n",
    "        self.y_hat = 0\n",
    "\n",
    "    def get_jacobian(self, h):\n",
    "        \"\"\" FILL THIS FUNCTION IN\n",
    "        \n",
    "        Calculates the Jacobian matrix of the network using a given\n",
    "        pre-activations vector h and the network, according to equation\n",
    "        above.\n",
    "        \n",
    "        Hints:\n",
    "        1) Be careful with numpy broadcasting rules!\n",
    "        2) Use the f_prime attribute of self.activation.\n",
    "        \n",
    "        Arguments:\n",
    "            h (numpy array, shape (n_h)): pre-activations\n",
    "            \n",
    "        Updates:\n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            J (numpy array, shape (n_h, n_h)): the Jacobian matrix\"\"\"\n",
    "        \n",
    "        ### TODO ###\n",
    "        ## Use equation ## to calculate the network\n",
    "        ## Jacobian for a given pre-activations vector\n",
    "        \n",
    "        J = 0\n",
    "\n",
    "        return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-SEL9J4bgdG"
   },
   "source": [
    "Below is a wrapper for the whole RNN training process. The simulate function takes in\n",
    "\n",
    "1. rnn (RNN instance)\n",
    "2. data (dict): A dictionary with two keys: 'train' and 'test', each of which points to a dict with keys 'X' and 'Y' for inputs and labels, respectively. \n",
    "3. mode (string): A string that should take on the value either 'train' or 'test', indicating whether to using training or testing data *and* whether to pause on each time step to run learn_alg.\n",
    "4. learn_alg (Learning_Algorithm instance)\n",
    "5. monitors (list): A list of strings indicating which rnn attributes you want to store the entire history of during a simulation\n",
    "6. T_max (int): Number of time steps to run loop for, if you don't want to go through all the data to save time.\n",
    "\n",
    "It then loops through the total number of time steps (dictated by the content of \"data\" or user-specified T_max), runs the network forwards at each time step, and then if mode is 'train', the learning algorithm is updated and applied. A dictionary \"mons\" stores all values of whatever the user wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gUMjhAMdbgdH"
   },
   "outputs": [],
   "source": [
    "def simulate(rnn, data, mode='test', learn_alg=None, monitors=['loss_', 'y_hat'], T_max=np.inf):\n",
    "        \n",
    "        x_inputs = data[mode]['X']\n",
    "        y_labels = data[mode]['Y']\n",
    "        total_time_steps = int(np.minimum(x_inputs.shape[0], T_max))\n",
    "        \n",
    "        #Set initial network state\n",
    "        rnn.reset_network()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        mons = {mon:[] for mon in monitors}\n",
    "        for i_t in range(total_time_steps):\n",
    "            \n",
    "            #Forward pass, get errors\n",
    "            x, y = x_inputs[i_t], y_labels[i_t]\n",
    "            rnn.next_state(x)\n",
    "            rnn.next_output()\n",
    "            rnn.loss_ = rnn.loss.f(rnn.z, y)\n",
    "            rnn.error = rnn.loss.f_prime(rnn.z, y)\n",
    "            \n",
    "            if mode == 'train':\n",
    "                \n",
    "                ## Update the memory content of the BPTT algorithm\n",
    "                learn_alg.update()\n",
    "\n",
    "                #Ever T_truncation time steps, update the network parameters\n",
    "                if i_t % learn_alg.T_truncation == 0 and i_t > 0:\n",
    "                \n",
    "                    \"\"\" You will write this function in the Learning Algorithm code block \"\"\"\n",
    "                    ## Use the algorithm to calculate a *list* of gradients, 5 in total,\n",
    "                    ## each of is a numpy array corresponding to one parameter:\n",
    "                    ## W_rec, W_in, b_rec, W_out, b_rec\n",
    "                    grads = learn_alg()\n",
    "\n",
    "                    #Update parameters\n",
    "                    for attr, grad in zip(['W_rec', 'W_in', 'b_rec', 'W_out', 'b_out'], grads):\n",
    "                        old_param_value = getattr(rnn, attr)\n",
    "                        new_param_value = old_param_value - learn_alg.lr * grad\n",
    "                        setattr(rnn, attr, new_param_value)\n",
    "            \n",
    "            \n",
    "            #IGNORE Update monitors\n",
    "            for key in mons:\n",
    "                try:\n",
    "                    mons[key].append(getattr(rnn, key))\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                \n",
    "            if i_t % 10000 == 0 and i_t > 0:\n",
    "                progress = np.round((i_t/total_time_steps)*100, 2)\n",
    "                time_elapsed = np.round(time.time() - start_time, 1)\n",
    "\n",
    "                summary = '\\rProgress: {}% complete \\nTime Elapsed: {}s \\n'\n",
    "                print(summary.format(progress, time_elapsed))\n",
    "\n",
    "        #IGNORE Convert monitors from lists into numpy arrays\n",
    "        for key in mons:\n",
    "            try:\n",
    "                mons[key] = np.array(mons[key])\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "        return rnn, mons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KaTWBr6bgdI"
   },
   "source": [
    "### Initialization\n",
    "Run a \"test\" run first before we try to train anything. Start by initializing a network object. We could choose any initial parameter values we want, but in practice some work better than others. I suggest the following:\n",
    "\n",
    "1. Initialize the bias vectors (recurrent and output) to 0\n",
    "2. Initialize the input weight matrix to sample iid from a Gaussian with 0 mean and standard deviation $1/\\sqrt{n_{\n",
    "\\text{in}}}$\n",
    "3. Initialize the output weight matrix in the same way but with standard deviation $1/\\sqrt{n_{\n",
    "\\text{hidden}}}$\n",
    "4. Initialize the reucrrent weight matrix as a random othogonal matrix. One way to do this: start with a random matrix sampling iid from N(0,1), and then perform a QR decomposition on it, taking the ${\\bf Q}$ matrix as the initial ${\\bf W}^{\\text{rec}}$. (Use the np.linalg.qr function.)\n",
    "\n",
    "### Task\n",
    "A note on the task. I have provided a data dictionary with training and test data, generated in the following way. The inputs are all iid Bernoulli samples with p = 0.5. (The second input dimension is simply the complement of the first, i.e. $x_1 = 1 - x_0$. We use input and output dimensions of 2 instead of 1 to make numpy broadcasting rules consistent.) The output has a baseline value of 0.5, which is increased by 0.5 if the input at 6 time steps ago is 1 and decreased by 0.25 if the input at 10 time steps back is 1. (The output is similarly 2-dimensional with a redundant second dimension.) Thus we have included 2 explicit intertemporal dependencies in the data. The network must continuously memorize the previous inputs and report the proper output.\n",
    "\n",
    "TODO:\n",
    "1. Initialize the network parameters and generate an initial RNN object. Then run a simulation in 'test' mode and plot the results.\n",
    "2. Except for code blocks marked with IGNORE, make sure you understand the simulate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C9iZlvZXbgdJ"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4167481997.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/cm/mgs9k2l93dj5mvvwvbb_79740000gn/T/ipykernel_16247/4167481997.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    W_in  = #shape = (n_hidden, n_in))\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with open('./task_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "n_in = 2\n",
    "n_hidden = 32\n",
    "n_out = 2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\" TODO: FILL IN PARAMETER INITIALIZATIONS \"\"\"\n",
    "#Initialize input/output weights as \n",
    "W_in  = #shape = (n_hidden, n_in))\n",
    "W_out = #shape = (n_out, n_hidden))\n",
    "\n",
    "#Initialize recurrent weights with a random *orthogonal* matrix\n",
    "W_rec = #shape = (n_hidden, n_hidden))\n",
    "\n",
    "#Initialize biases to 0\n",
    "b_rec = #shape = (n_hidden)\n",
    "b_out = #shape = (n_out)\n",
    "\n",
    "#Initialize RNN object with these initial weights, \\phi = tanh, \\psi = I, L = MSE\n",
    "rnn = RNN(W_in, W_rec, W_out, b_rec, b_out,\n",
    "         activation=tanh,\n",
    "         output=identity,\n",
    "         loss=mean_squared_error)\n",
    "\n",
    "rnn, mons = simulate(rnn, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3aBzW4YbgdK"
   },
   "outputs": [],
   "source": [
    "### --- PLOT RESULTS --- ###\n",
    "plt.plot(mons['y_hat'][:,0])\n",
    "plt.plot(data['test']['Y'][:,0])\n",
    "plt.legend(['Prediction', 'Label'])\n",
    "plt.xlim([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtdqcCyRbgdK"
   },
   "outputs": [],
   "source": [
    "mons['y_hat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNoc5fAbbgdL"
   },
   "outputs": [],
   "source": [
    "x = mons['y_hat'].flatten()\n",
    "y = data['test']['Y'].flatten()\n",
    "plt.plot(x, y, '.', alpha=0.05)\n",
    "plt.plot([np.amin(x), np.amax(x)],\n",
    "          [np.amin(y), np.amax(y)], 'k', linestyle='--')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0SmaNhDbgdL"
   },
   "source": [
    "Of course, this untrained network is useless. So let's try training by BPTT, using the \"credit assignment vector\"\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf q}^{(t)} \\equiv \\partial \\mathcal{L} / \\partial {\\bf a}^{(t)}.\n",
    "\\end{align}\n",
    "\n",
    "We can unpack this expression going backwards via\n",
    "\n",
    "\\begin{align}\n",
    "{\\bf c}^{(t'-1)} = {\\bf q}^{(t'-1)} + {\\bf c}^{(t')} {\\bf J}^{(t')}\n",
    "\\end{align}\n",
    "\n",
    "for any $t'$ in the range from $t - T$ to $t$, where $T$ is the trunction horizon. We start with ${\\bf c}^{(t)} = {\\bf q}^{(t)}$, calculate each of the ${\\bf c}$ values going backwards, and then get an estimate of the gradient at each time step $t'$\n",
    "\n",
    "\\begin{align}\n",
    "\\partial \\mathcal{L} / \\partial W^{(t')}_{ij} &= {\\bf c}^{(t')}_i \\partial a^{(t')}_i / \\partial W^{(t')}_{ij}\\\\\n",
    "&= {\\bf c}^{(t')}_i \\phi'(h^{(t)}_i) \\hat{a}^{(t-1)}_j\n",
    "\\end{align}\n",
    "\n",
    "We finally sum these up and return it as the gradient.\n",
    "\n",
    "TODO\n",
    "1. Update the ${\\bf c}$s going backwards in the get_rec_grads method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "puHXul7QbgdM"
   },
   "outputs": [],
   "source": [
    "class Learning_Algorithm:\n",
    "    \n",
    "    \"\"\"Parent class for all types of learning algorithms.\n",
    "    Attributes:\n",
    "        rnn (network.RNN): An instance of RNN to be trained by the network.\n",
    "        n_* (int): Extra pointer to net.n_* (in, h, out) for conveneince.\n",
    "        m (int): Number of recurrent \"input dimensions\" n_h + n_in + 1 including\n",
    "            task inputs and constant 1 for bias.\n",
    "        q (numpy array): Array of immediate error signals for the hidden units,\n",
    "            i.e. the derivative of the current loss with respect to net.a, of\n",
    "            shape (n_h).\"\"\"\n",
    "    \n",
    "    def __init__(self, rnn, learning_rate=0.001):\n",
    "        \n",
    "        #Define basic learning algorithm properties\n",
    "        self.rnn = rnn\n",
    "        self.n_in = self.rnn.n_in\n",
    "        self.n_h = self.rnn.n_h\n",
    "        self.n_out = self.rnn.n_out\n",
    "        self.m = self.n_h + self.n_in + 1\n",
    "        self.q = np.zeros(self.n_h)\n",
    "        self.lr = learning_rate\n",
    "        self.outer_grads = 0\n",
    "\n",
    "    def get_outer_grads(self):\n",
    "        \"\"\"Calculates the derivative of the loss with respect to the output\n",
    "        parameters net.W_out and net.b_out.\n",
    "\n",
    "        Calculates the outer gradients in the manner of a perceptron derivative\n",
    "        by taking the outer product of the error with the \"regressors\" onto the\n",
    "        output (the hidden state and constant 1).\n",
    "\n",
    "        Returns:\n",
    "            A numpy array of shape (net.n_out, self.n_h + 1) containing the\n",
    "                concatenation (along column axis) of the derivative of the loss\n",
    "                w.r.t. net.W_out and w.r.t. net.b_out.\"\"\"\n",
    "\n",
    "        self.a_ = np.concatenate([self.rnn.a, np.array([1])])\n",
    "        return np.multiply.outer(self.rnn.error, self.a_)\n",
    "\n",
    "    def propagate_feedback_to_hidden(self):\n",
    "        \"\"\"Performs one step of backpropagation from the outer-layer errors to\n",
    "        the hidden state.\n",
    "\n",
    "        Calculates the immediate derivative of the loss with respect to the\n",
    "        hidden state rnn.a.\n",
    "\n",
    "        Updates q to the current value of dL/da.\"\"\"\n",
    "        \n",
    "        self.q = self.rnn.error.dot(self.rnn.W_out)\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Calculates the final list of grads for this time step.\n",
    "\n",
    "        Assumes the user has already called self.update()), a\n",
    "        method specific to each child class. Uses the accumulated\n",
    "        outer_grads (and resets to 0 after), then calls\n",
    "        get_rec_grads method (specific to each child class) to get the gradients\n",
    "        of W_rec, W_in, and b_rec as one numpy array with shape (n_h, m). Then\n",
    "        these gradients are split along the column axis into a list of 5\n",
    "        gradients for W_rec, W_in, b_rec, W_out, b_out.\n",
    "\n",
    "        Returns:\n",
    "            List of gradients for W_rec, W_in, b_rec, W_out, b_out.\"\"\"\n",
    "        \n",
    "        self.rec_grads = self.get_rec_grads()\n",
    "        rec_grads_list = split_weight_matrix(self.rec_grads,\n",
    "                                             [self.n_h, self.n_in, 1])\n",
    "        outer_grads_list = split_weight_matrix(self.outer_grads,\n",
    "                                               [self.n_h, 1])\n",
    "        grads_list = rec_grads_list + outer_grads_list\n",
    "\n",
    "        #Reset outer grads to 0\n",
    "        self.outer_grads = 0\n",
    "        \n",
    "        return grads_list\n",
    "    \n",
    "class BPTT(Learning_Algorithm):\n",
    "    \n",
    "    def __init__(self, rnn, T_truncation, learning_rate=0.001):\n",
    "        \n",
    "        super().__init__(rnn, learning_rate=learning_rate)\n",
    "        \n",
    "        self.T_truncation = T_truncation\n",
    "        \n",
    "        #Initialize lists for storing network data\n",
    "        self.a_hat_history = [np.zeros(self.m)] * self.T_truncation\n",
    "        self.h_history = [np.zeros(self.n_h)] * self.T_truncation\n",
    "        self.q_history = [np.zeros(self.n_h)] * self.T_truncation\n",
    "        \n",
    "    def update(self):\n",
    "        \n",
    "        #Add latest values to list\n",
    "        self.a_hat = np.concatenate([self.rnn.a_prev,\n",
    "                                     self.rnn.x,\n",
    "                                     np.array([1])])\n",
    "        \n",
    "        self.a_hat_history.append(self.a_hat)\n",
    "        \n",
    "        self.h_history.append(self.rnn.h)\n",
    "        \n",
    "        self.propagate_feedback_to_hidden()\n",
    "        self.q_history.append(self.q)\n",
    "        \n",
    "        #Delete old values \n",
    "        del(self.a_hat_history[0])\n",
    "        del(self.h_history[0])\n",
    "        del(self.q_history[0])\n",
    "        \n",
    "        #Accumulate outer grads\n",
    "        self.outer_grads += self.get_outer_grads()\n",
    "        \n",
    "    def get_rec_grads(self):\n",
    "        \n",
    "        \"\"\" FILL IN FUNCTION \n",
    "        \n",
    "        Using the accumulated history of q, h and a_hat values,\n",
    "        calculate the gradient of W\n",
    "        \n",
    "        Returns:\n",
    "            rec_grads (numpy array, (n_h, n_h + n_in + 1)): Gradient \\partial L / \\partial W as computed\n",
    "                by backpropagating through the unrolled graph.\n",
    "        \"\"\"\n",
    "\n",
    "        #Hint: initialize the desired output to 0\n",
    "        rec_grads = 0\n",
    "        #Hint: initialize first c value to the last q value\n",
    "        c = self.q_history[-1]\n",
    "        for i_BPTT in range(self.T_truncation):\n",
    "            \n",
    "            #Access present values of h and a_hat\n",
    "            h = self.h_history[-(i_BPTT + 1)]\n",
    "            a_hat = self.a_hat_history[-(i_BPTT + 1)]\n",
    "            \n",
    "            #Get immediate influence\n",
    "            D = self.rnn.activation.f_prime(h)\n",
    "            M = np.multiply.outer(D, a_hat)\n",
    "            \n",
    "            rec_grads += (c * M.T).T\n",
    "            \n",
    "            if i_BPTT == self.T_truncation - 1:\n",
    "                continue\n",
    "            \n",
    "            q = self.q_history[-(i_BPTT + 2)]\n",
    "            J = self.rnn.get_jacobian(h=h)\n",
    "            # TODO: update cs\n",
    "            c = c\n",
    "        \n",
    "        return rec_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ALo8yzFUbgdM"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cm/mgs9k2l93dj5mvvwvbb_79740000gn/T/ipykernel_16247/3081887993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Initialize input/output weights as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "n_in = 2\n",
    "n_hidden = 32\n",
    "n_out = 2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\" TODO: FILL IN PARAMETER INITIALIZATIONS \"\"\"\n",
    "#Initialize input/output weights as \n",
    "W_in  = #shape = (n_hidden, n_in))\n",
    "W_out = #shape = (n_out, n_hidden))\n",
    "\n",
    "#Initialize recurrent weights with a random *orthogonal* matrix\n",
    "W_rec = #shape = (n_hidden, n_hidden))\n",
    "\n",
    "#Initialize biases to 0\n",
    "b_rec = #shape = (n_hidden)\n",
    "b_out = #shape = (n_out)\n",
    "\n",
    "#Initialize RNN object with these initial weights, \\phi = tanh, \\psi = I, L = MSE\n",
    "rnn = RNN(W_in, W_rec, W_out, b_rec, b_out,\n",
    "          activation=tanh,\n",
    "          output=identity,\n",
    "          loss=mean_squared_error)\n",
    "\n",
    "T_truncation = 15\n",
    "learning_rate = 0.001\n",
    "learn_alg = BPTT(rnn, T_truncation=T_truncation, learning_rate=learning_rate)\n",
    "\n",
    "rnn, mons = simulate(rnn, data, mode='train', learn_alg=learn_alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-OkWUhtbgdN"
   },
   "outputs": [],
   "source": [
    "### --- PLOT TRAINING LOSS --- ###\n",
    "filtered_loss = decimate(decimate(mons['loss_'], 10), 10)\n",
    "plt.plot(filtered_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2JIsSo1bgdN"
   },
   "outputs": [],
   "source": [
    "### --- RUN TEST SIMULATION --- ###\n",
    "rnn, mons = simulate(rnn, data, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf45cQDjbgdN"
   },
   "outputs": [],
   "source": [
    "### --- PLOT RESULTS --- ###\n",
    "plt.figure()\n",
    "plt.plot(mons['y_hat'][:,0])\n",
    "plt.plot(data['test']['Y'][:,0])\n",
    "plt.legend(['Prediction', 'Label'])\n",
    "plt.xlim([100, 200])\n",
    "plt.figure()\n",
    "x = mons['y_hat'].flatten()\n",
    "y = data['test']['Y'].flatten()\n",
    "plt.plot(x, y, '.', alpha=0.05)\n",
    "plt.plot([np.amin(x), np.amax(x)],\n",
    "          [np.amin(y), np.amax(y)], 'k', linestyle='--')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC7wosV4bgdO"
   },
   "source": [
    "## Please turn in the code and pdf before 12/05/2022 at 11:59 pm. Please name your notebook netid.ipynb.\n",
    "\n",
    "### Your work will be evaluated based on the code and plots. You don't need to write down your answers to these questions in the text blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J2ZAxWBbgdO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lab9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('PTSA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3f6ccb4d9c2992e26314c6a55f25a363977377544d6a8d30ed481869f911c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
